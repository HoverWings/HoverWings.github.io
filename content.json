{"meta":{"title":"HoverWings' Site","subtitle":null,"description":null,"author":"Xiang Pan","url":"hoverwings.site","root":"/"},"pages":[{"title":"Categories","date":"2019-06-18T18:16:14.653Z","updated":"2019-06-18T17:14:07.838Z","comments":true,"path":"categories/index.html","permalink":"hoverwings.site/categories/index.html","excerpt":"","text":""},{"title":"About","date":"2019-06-18T18:16:14.649Z","updated":"2019-06-18T17:14:07.838Z","comments":true,"path":"about/index.html","permalink":"hoverwings.site/about/index.html","excerpt":"","text":""},{"title":"Tags","date":"2019-06-18T18:16:14.650Z","updated":"2019-06-18T17:14:07.839Z","comments":true,"path":"tags/index.html","permalink":"hoverwings.site/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Learning Path","slug":"Learning-Path","date":"2019-06-20T07:15:10.000Z","updated":"2019-06-20T07:16:02.144Z","comments":true,"path":"2019/06/20/Learning-Path/","link":"","permalink":"hoverwings.site/2019/06/20/Learning-Path/","excerpt":"","text":"参考资料","categories":[],"tags":[]},{"title":"GN_Application","slug":"GN-Application","date":"2019-06-20T04:40:00.000Z","updated":"2019-06-26T08:53:24.396Z","comments":true,"path":"2019/06/20/GN-Application/","link":"","permalink":"hoverwings.site/2019/06/20/GN-Application/","excerpt":"","text":"Reference","categories":[{"name":"Graph_Network","slug":"Graph-Network","permalink":"hoverwings.site/categories/Graph-Network/"}],"tags":[]},{"title":"RS_RL","slug":"RS-RL","date":"2019-06-19T15:00:32.000Z","updated":"2019-06-19T17:38:55.060Z","comments":true,"path":"2019/06/19/RS-RL/","link":"","permalink":"hoverwings.site/2019/06/19/RS-RL/","excerpt":"","text":"参考资料","categories":[{"name":"Recommendation_System","slug":"Recommendation-System","permalink":"hoverwings.site/categories/Recommendation-System/"}],"tags":[]},{"title":"RS_GAN","slug":"RS-GAN","date":"2019-06-18T18:20:49.000Z","updated":"2019-06-24T13:52:19.090Z","comments":true,"path":"2019/06/19/RS-GAN/","link":"","permalink":"hoverwings.site/2019/06/19/RS-GAN/","excerpt":"","text":"SeqGAN Goal: generating sequences of discrete tokens. Difficulty - pass the gradient update from the discriminative model to the generative model. - discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated Related Work To address this problem, (Bengio et al. 2015) proposed a training strategy called scheduled sampling (SS) p真实+(1-p)非真实 p随着训练逐渐减小 模型越来越好，可以逐渐用生成的 (Husz´ar 2015) showed that SS is an inconsistent training strategy and fails to address the problem fundamentally. GAN Challenge discrete Firstly, GAN is designed for generating realvalued, continuous data but has difﬁculties in directly generating sequences of discrete tokens, such as texts (Husz´ar 2015). “slight change” gradient As such, the gradient of the loss from D w.r.t. the outputs by G is used to guide the generative model G (paramters) to slightly change the generated value to make it more realistic. If the generated data is based on discrete tokens, the “slight change” guidance from the discriminative net makes little sense because there is probably no corresponding token for such slight change in the limited dictionary space (Goodfellow 2016). SeqGAN State 1）在GAN中，Generator是通过随机抽样作为开始，然后根据模型的参数进行确定性的转化。通过generative model G的输出，discriminative model D计算的损失值，根据得到的损失梯度去指导generative model G做轻微改变，从而使G产生更加真实的数据。而在文本生成任务中，G通常使用的是LSTM，那么G传递给D的是一堆离散值序列，即每一个LSTM单元的输出经过softmax之后再取argmax或者基于概率采样得到一个具体的单词，那么这使得梯度下架很难处理。 2）GAN只能评估出整个生成序列的score/loss，不能够细化到去评估当前生成token的好坏和对后面生成的影响。 强化学习可以很好的解决上述的两点。再回想一下Policy Gradient的基本思想，即通过reward作为反馈，增加得到reward大的动作出现的概率，减小reward小的动作出现的概率，如果我们有了reward，就可以进行梯度训练，更新参数。如果使用Policy Gradient的算法，当G产生一个单词时，如果我们能够得到一个反馈的Reward，就能通过这个reward来更新G的参数，而不再需要依赖于D的反向传播来更新参数，因此较好的解决了上面所说的第一个屏障。对于第二个屏障，当产生一个单词时，我们可以使用蒙塔卡罗树搜索(Alpho Go也运用了此方法)立即评估当前单词的好坏,而不需要等到整个序列结束再来评价这个单词的好坏。 G CNN+ HighwayNetwork roll-out policy，这个策略也是通过一个神经网络实现，这个神经网络我们可以认为就是我们的Generator。得到采样的序列后，我们把这一堆序列扔给Discriminator，得到一批输出为1的概率，这堆概率的平均值即我们的reward。 D Evaluation preTrained的LSTM作为标准(as human observer) Train IRGAN SIGIR2017 r: rank-order label/ score G-optimise D-optimise CFGAN Z:nosie C:User Vec mask: mask向量eu中，用户交互过的地方为1，没有交互过的地方为0 - 为0的地方并不代表用户不喜欢，其次，用户没有购买过的物品，在乘上0之后，其实是训练的时候丢掉了这些节点，因为乘上0之后，梯度永远为0，是不会反向传播回去的； - 如果把真实购买向量对应位置设置为0的话，判别器在这些地方也是没有损失的。这与传统的矩阵分解方法是非常类似的，即用用户购买过的地方去训练，随后预测用户对未购买过的物品的评分。 CFGAN−ZR Negative Sampling CFGAN−PM partial-mask G: P-&gt;1 N-&gt;0 CFGAN−ZP GraphGAN Sigmoid/SDNE any D Graph Softmax 仅针对直接使用softmax作为G提供Graph-Softmax proof 先到叶子节点，建立新树，向上走计算概率 Thoughts 直接用GAN做补充样本是无用的 GAN1:一套G/D网络进行序列产生判别 作为模拟环境 GAN2:一套G/D网络进行购物券产生 作为购物券生成器 先用G1/D1拟合现有的分布 seqGAN seq-&gt;G1-&gt;full seq (i-c-i-i)-&gt;(i-c-i-i-c-i) D进行判别这个分布是否有效(真实) 利用G网络产生购物券种类 利用D网络判别购物券产生的有效性 利用conditional GAN产生特定的购物券？ 再用G2/D2拟合购物券 购物券存在embedding (store,catgrary,user,购物券的组合(平台券/档口券(store),方式，金额))-&gt;G2-&gt;-&gt;embedding [(store,catgrary,user,购物券的组合(平台券/档口券(store),方式，金额)),embedding] -&gt;D2-&gt;true/false real embedding()-&gt;D2-&gt;true 真实： D2网络判别为真实购物券 D1网络判别序列是否真实 有效： 购买时间-- 购买金额++ 问题 商家从来没这么发过购物券，是否会真实判定有问题 是否不需要GAN2 直接用MLP 生成购物券组合 embedding (梯度问题) Reference SeqGAN GraphGAN IRGAN CFGAN","categories":[{"name":"Recommendation_System","slug":"Recommendation-System","permalink":"hoverwings.site/categories/Recommendation-System/"}],"tags":[]},{"title":"RS_Explainability","slug":"RS-Explainability","date":"2019-06-07T15:56:11.000Z","updated":"2019-06-19T17:34:29.940Z","comments":true,"path":"2019/06/07/RS-Explainability/","link":"","permalink":"hoverwings.site/2019/06/07/RS-Explainability/","excerpt":"","text":"PanXiang 2019.4.9 Problem why such items are recommended why they not only provideusers with the recommendations effectiveness efficiency persuasiveness user satisfaction (how presented)display style:text/visual (what information used)model:matrix factorization,topic modeling, graphbased models, deep learning, knowledge-graph embedding, association rule mining Time Line CF(Memory-Based) C ontent- B ased CF(feature based) (price, color, brand of the goods in e-commerce, or the genre, director, duration of the movies inreview systems) User-based CF (user embedding?) Item-based CF(Item embedding?) MF(Model-Based) 生成式模型/判别式模型? MF LFM(Latent Factor Model) m user n item A: user-(item favor value) U: user-feature V: feature-(itemfavor value) MF LFM(Latent Factor Model) /Singular Value Decomposition(SVD)? m user n item A: user-(item favor value) U: user-feature V: feature-(itemfavor value) yi反应喜好偏置 I(u)所有交互 behaviour can reflect favor 2008 KDD Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model E FM(Explicit Factor Model) +explicit user opinions explicit product features user opinions by phrase-level sentiment analysis on user reviews generate according to specific product features to the user’s interests and the hidden features learned SIGIR 2014Explicit Factor Models for Explainable Recommendation based on Phrase-level Sentiment Analysis EFM(Explicit Factor Model) SIGIR 2014Explicit Factor Models for Explainable Recommendation based on Phrase-level Sentiment Analysis EFM(Explicit Factor Model) N=5 for five stars feature Fj is mentioned by user ui for tij times Fj is mentioned for k times on item pi SIGIR 2014Explicit Factor Models for Explainable Recommendation based on Phrase-level Sentiment Analysis EFM(Explicit Factor Model) Explicit features feature: screen/earphone factor: ??? X:user-feature attention matrix Y:item-feature quality matrix U1 user- factor V feature-factor U2 item- factor r factor nums m user p feature n item SIGIR 2014Explicit Factor Models for Explainable Recommendation based on Phrase-level Sentiment Analysis EFM(Explicit Factor Model) Latent f actors When r = 0, this model reduces to a traditional latent factorization model on user-item rating matrix A which means that the explicit features are not used for recommendations EFM(Explicit Factor Model) +SVD++? SIGIR’18 Explainable Recommendation via Multi-Task Learning in Opinionated Text Data ConvMF=CNN+PMF PMF R: score Mat posterior probability M item N doc 2016Convolutional matrix factorization for document context-aware recommendation CNN: (Music:Fourier-&gt;image-&gt;CNN predict Genetrate Feature) Conv -Classification-&gt;feature extraction MF -Regression 2016Convolutional matrix factorization for document context-aware recommendation Z[t]:update gate vector R[t]:reset gate vector RecSys17 Sequential User-based Recurrent Neural Network Recommendations RecSys17 Sequential User-based Recurrent Neural Network Recommendations Text Sentence: Topic words:(features) Visual Friends generated character by character concatenated word embeddings of user, item, and rating 2018Automatic Generation of Natural Language Explanations Text-based(NLP) 2018Automatic Generation of Natural Language Explanations Text-based(NLP) 2018Automatic Generation of Natural Language Explanations Knowledge-base =path comlpetion 2019Unifying Knowledge Graph Learning and Recommendation: Towards a Better Understanding of User Preferences Knowledge-base AAAI2019Unifying Knowledge Graph Learning and Recommendation: Towards a Better Understanding of User Preferences Knowledge-base AAAI2019Unifying Knowledge Graph Learning and Recommendation: Towards a Better Understanding of User Preferences Knowledge-base TransE TransH It assumes that each relation owns a hyperplane, and the translation between head entity and tail entity is valid only if they are projected to the same hyperplane WWW2019Unifying Knowledge Graph Learning and Recommendation:Towards a Better Understanding of User Preferences Knowledge-base TransH WWW2019Unifying Knowledge Graph Learning and Recommendation:Towards a Better Understanding of User Preferences Evaluation-RS reli = 1 if the i-th element is a positive item Evaluation -Explainable explainability EP:explainability precision ER:explainabilityrecall Text: BLUE: 不同长度的统计值差异计算 Rough(Recall-Oriented Understudy for Gisting Evaluation) LCSLongest Child String Open Question DL explainability Heterogenous Information Modeling Text/Image/Audio Search Natural Language Generation for Explanation QA System Explanation beyond Persuasiveness letting the user know why not to buy a certain product the system can help to save time for the users and to win user’s trustin the system Open Question Aggregation of Different Explanations Explainable ML Explainable D L Understanding Black-box Predictions via Influence Functions","categories":[{"name":"Recommendation_System","slug":"Recommendation-System","permalink":"hoverwings.site/categories/Recommendation-System/"}],"tags":[]},{"title":"ML in Complex Netetwork","slug":"ML-in-Complex-Netetwork","date":"2019-06-04T06:34:18.000Z","updated":"2019-06-29T09:42:08.069Z","comments":true,"path":"2019/06/04/ML-in-Complex-Netetwork/","link":"","permalink":"hoverwings.site/2019/06/04/ML-in-Complex-Netetwork/","excerpt":"","text":"Machine Learning in Complex Network Xiang Pan Supervised By: Hong Huang 6/2/2019 --- Complex Network A Complex Network is a graph (network) with non-trivial topological features—features that do not occur in simple networks such as lattices or random graphs but often occur in graphs modelling of real systems. Node and Link:The general model for the world CS?:Compute and Storage Traditional Method Dynamics Deep Walk Population Predication Propagation Theory Infectious Diseases Propagation Cluster Theory Aggregation Community Dection ML+Complex Network More information than Vector Build and represent do not loss critical information porpertity structure Adapt to more general dataset type More Method Graph CNN/DNN/VAE/GAN Pipeline ML PipeLine 1234digraph G&#123; &quot;Select Topic&quot;-&gt;&quot;Preparing Data Set(Data Clean)&quot;-&gt;&quot;Feature Engineering&quot;-&gt;&quot;Bilud Model&quot;&#125; Build Network Defination Node \\[N=O(Node)\\] can be the anything of the world The understanding Link Similarity And Diffidence \\[{ V*V \\in R }\\] \\[{ S_{ij}=s(v_i,v_j) }\\] The Similarity Function can be the diastance of vector representation \\[{ D_{ij}=d(v_i,v_j) }\\] The Diffidence Function can be calculated by the Similarity Function \\[ { d(v_i,v_j)=\\sqrt {s(v_i,v_i)+s(v_j,v_j)-2s(v_i,v_j)} } \\] \\[{ P_{ij}=\\alpha * S_{ij} + \\beta *1/D_{ij} }\\] Then set a threshhold to build the link How to determine the threshhold(hyperparameter)? Network sparsity(Experience) &gt;After obtaining the base image matrix and coefficient matrix of non-negative matrix factorization (NMF), Hoyer [1] proposed that the degree of difference between the L1 norm and the L2 norm can be used to measure the sparsity of the matrix after decomposition. Training result Auto ML Representataion Network Representation:Using some method to reduce the dimension of the data, then the network can be represent as a Vector - Graph Embedding MF M is an Orthogonal matrix ### SVD \\[ M \\in Space_{node * node} \\] \\(U\\) is the main Vector representation \\(V= \\Sigma * V^*\\) is the Background Vector Repersentation Deep Walk Hierarchical Huffman Tree Can be understood by MF? PTE node2vec — 2nd Order Random Walk node2vec as Implicit Matrix Factorization Everything is MF NetMF Application Abstract Link Prediction Clustering Dynamic Network Analysis Real World Infectious Diseases Academic Network Media Web Recommendation System(RS) Terrorist Dection Recommendation System Reference Jie Tang's Tutorial in WWW 2019 Machine Leaning in Complex Network","categories":[{"name":"Graph_Network","slug":"Graph-Network","permalink":"hoverwings.site/categories/Graph-Network/"}],"tags":[]},{"title":"RS_Transformer","slug":"RS-Transformer","date":"2019-06-03T18:25:12.000Z","updated":"2019-06-19T17:34:29.940Z","comments":true,"path":"2019/06/04/RS-Transformer/","link":"","permalink":"hoverwings.site/2019/06/04/RS-Transformer/","excerpt":"","text":"Behavior Sequence Transformer for E-commerce Recommendation in Alibaba Abstract Embedding&amp;MLP paradigm: raw features are embedded into lowdimensional vectors ignoring the sequential nature of users’ behaviors Transformer model to capture the sequential signals underlying users’ behavior sequences for recommendation Transforemer(Attention is all you need) Architecture Every encoder do not share their weights Self-Attention Encoder Input Word-&gt;Embededing \\[ q_1=V*W_q \\] \\[ k_1=V*W_k \\] \\[ v_1=V*W_v \\] Position: diffierent distance to different pos Decoder The Unis number is O(Word) loss The differience of 2 distribution - Cross Entropy - KL Distance BST Architecture Feature General Embedding Positional embedding \\[pos(v_i) = t(v_t) − t(v_i)\\] where \\(t(v_t)\\) represents the recommending time and \\(t(v_i)\\) the timestamp when user click item \\(v_i\\). Transformer Self-attention layer Point-wise Feed-Forward Networks MLP layers and Loss function binary classification problem Result","categories":[{"name":"Recommendation_System","slug":"Recommendation-System","permalink":"hoverwings.site/categories/Recommendation-System/"}],"tags":[]}]}