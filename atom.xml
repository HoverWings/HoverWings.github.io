<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>HoverWings&#39; Site</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="hoverwings.site/"/>
  <updated>2019-06-29T10:54:25.531Z</updated>
  <id>hoverwings.site/</id>
  
  <author>
    <name>Xiang Pan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Few-Shot Learning</title>
    <link href="hoverwings.site/2019/06/29/Few-Shot-Learning/"/>
    <id>hoverwings.site/2019/06/29/Few-Shot-Learning/</id>
    <published>2019-06-29T10:53:02.000Z</published>
    <updated>2019-06-29T10:54:25.531Z</updated>
    
    <content type="html"><![CDATA[<p>For exploring the posibility of sovling the cold-start problem in RS, I try to survey the Few-Shot learning.</p><h2 id="rederence">Rederence</h2><blockquote><ul><li><a href></a></li><li><a href></a></li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;For exploring the posibility of sovling the cold-start problem in RS, I try to survey the Few-Shot learning.&lt;/p&gt;
&lt;h2 id=&quot;rederence&quot;&gt;Reder
      
    
    </summary>
    
      <category term="Meta_Leaning" scheme="hoverwings.site/categories/Meta-Leaning/"/>
    
    
  </entry>
  
  <entry>
    <title>Learning Path</title>
    <link href="hoverwings.site/2019/06/20/Learning-Path/"/>
    <id>hoverwings.site/2019/06/20/Learning-Path/</id>
    <published>2019-06-20T07:15:10.000Z</published>
    <updated>2019-06-20T07:16:02.144Z</updated>
    
    <content type="html"><![CDATA[<iframe id="embed_dom" name="embed_dom" frameborder="0" style="display:block;width:1024px; height:768px;" src="https://www.processon.com/embed/mind/5d0b13c9e4b024123de1ccf7"></iframe><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href></a></li><li><a href></a></li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;iframe id=&quot;embed_dom&quot; name=&quot;embed_dom&quot; frameborder=&quot;0&quot; style=&quot;display:block;width:1024px; height:768px;&quot; src=&quot;https://www.processon.com/emb
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>GN_Application</title>
    <link href="hoverwings.site/2019/06/20/GN-Application/"/>
    <id>hoverwings.site/2019/06/20/GN-Application/</id>
    <published>2019-06-20T04:40:00.000Z</published>
    <updated>2019-06-26T08:53:24.396Z</updated>
    
    <content type="html"><![CDATA[<h2 id="reference">Reference</h2><blockquote><ul><li><a href></a></li><li><a href></a></li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

      
    
    </summary>
    
      <category term="Graph_Network" scheme="hoverwings.site/categories/Graph-Network/"/>
    
    
  </entry>
  
  <entry>
    <title>RS_RL</title>
    <link href="hoverwings.site/2019/06/19/RS-RL/"/>
    <id>hoverwings.site/2019/06/19/RS-RL/</id>
    <published>2019-06-19T15:00:32.000Z</published>
    <updated>2019-06-19T17:38:55.060Z</updated>
    
    <content type="html"><![CDATA[<h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href></a></li><li><a href></a></li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;参考资料&quot;&gt;参考资料&lt;/h2&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

      
    
    </summary>
    
      <category term="Recommendation_System" scheme="hoverwings.site/categories/Recommendation-System/"/>
    
    
  </entry>
  
  <entry>
    <title>RS_GAN</title>
    <link href="hoverwings.site/2019/06/19/RS-GAN/"/>
    <id>hoverwings.site/2019/06/19/RS-GAN/</id>
    <published>2019-06-18T18:20:49.000Z</published>
    <updated>2019-06-24T13:52:19.090Z</updated>
    
    <content type="html"><![CDATA[<h1 id="seqgan">SeqGAN</h1><p><img src="/2019/06/19/RS-GAN/2019-06-24-16-38-17.png"></p><hr><p>Goal: generating sequences of discrete tokens.</p><p>Difficulty - pass the gradient update from the discriminative model to the generative model. - discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated</p><hr><h2 id="related-work">Related Work</h2><p>To address this problem, (Bengio et al. 2015) proposed a training strategy called scheduled sampling (SS)</p><p>p真实+(1-p)非真实 p随着训练逐渐减小 模型越来越好，可以逐渐用生成的</p><p>(Husz´ar 2015) showed that SS is an inconsistent training strategy and fails to address the problem fundamentally.</p><hr><h2 id="gan-challenge">GAN Challenge</h2><ul><li>discrete Firstly, GAN is designed for generating realvalued, continuous data but has difﬁculties in directly generating sequences of discrete tokens, such as texts (Husz´ar 2015).</li><li>“slight change” gradient As such, the gradient of the loss from D w.r.t. the outputs by G is used to guide the generative model G (paramters) to slightly change the generated value to make it more realistic. If the generated data is based on discrete tokens, the “slight change” guidance from the discriminative net makes little sense because there is probably no corresponding token for such slight change in the limited dictionary space (Goodfellow 2016).</li></ul><hr><h2 id="seqgan-1">SeqGAN</h2><p><img src="/2019/06/19/RS-GAN/2019-06-24-16-45-03.png"></p><hr><p>State <img src="/2019/06/19/RS-GAN/2019-06-24-16-48-36.png"></p><p><img src="/2019/06/19/RS-GAN/2019-06-24-16-46-11.png"></p><hr><p>1）在GAN中，Generator是通过随机抽样作为开始，然后根据模型的参数进行确定性的转化。通过generative model G的输出，discriminative model D计算的损失值，根据得到的损失梯度去指导generative model G做轻微改变，从而使G产生更加真实的数据。而在文本生成任务中，G通常使用的是LSTM，那么G传递给D的是一堆离散值序列，即每一个LSTM单元的输出经过softmax之后再取argmax或者基于概率采样得到一个具体的单词，那么这使得梯度下架很难处理。</p><p>2）GAN只能评估出整个生成序列的score/loss，不能够细化到去评估当前生成token的好坏和对后面生成的影响。</p><hr><p>强化学习可以很好的解决上述的两点。再回想一下Policy Gradient的基本思想，即通过reward作为反馈，增加得到reward大的动作出现的概率，减小reward小的动作出现的概率，如果我们有了reward，就可以进行梯度训练，更新参数。如果使用Policy Gradient的算法，当G产生一个单词时，如果我们能够得到一个反馈的Reward，就能通过这个reward来更新G的参数，而不再需要依赖于D的反向传播来更新参数，因此较好的解决了上面所说的第一个屏障。对于第二个屏障，当产生一个单词时，我们可以使用蒙塔卡罗树搜索(Alpho Go也运用了此方法)立即评估当前单词的好坏,而不需要等到整个序列结束再来评价这个单词的好坏。</p><hr><h3 id="g">G</h3><p><img src="/2019/06/19/RS-GAN/2019-06-24-17-12-03.png"> <img src="/2019/06/19/RS-GAN/2019-06-24-17-12-16.png"> CNN+ HighwayNetwork roll-out policy，这个策略也是通过一个神经网络实现，这个神经网络我们可以认为就是我们的Generator。得到采样的序列后，我们把这一堆序列扔给Discriminator，得到一批输出为1的概率，这堆概率的平均值即我们的reward。</p><hr><h3 id="d">D</h3><p><img src="/2019/06/19/RS-GAN/2019-06-24-17-13-06.png"> <img src="/2019/06/19/RS-GAN/2019-06-24-17-14-11.png"></p><p><img src="/2019/06/19/RS-GAN/2019-06-24-17-02-26.png"></p><hr><h3 id="evaluation">Evaluation</h3><p>preTrained的LSTM作为标准(as human observer)</p><hr><p><img src="/2019/06/19/RS-GAN/2019-06-24-17-04-13.png"></p><hr><p><img src="/2019/06/19/RS-GAN/2019-06-24-17-16-25.png"></p><hr><h3 id="train">Train</h3><p><img src="/2019/06/19/RS-GAN/2019-06-24-17-09-46.png"></p><hr><h1 id="irgan">IRGAN</h1><p>SIGIR2017 <img src="/2019/06/19/RS-GAN/2019-06-24-20-24-51.png"></p><hr><p>r: rank-order label/ score</p><p><img src="/2019/06/19/RS-GAN/2019-06-24-17-56-45.png"></p><hr><p><img src="/2019/06/19/RS-GAN/2019-06-24-20-16-22.png"></p><hr><h2 id="g-optimise">G-optimise</h2><p><img src="/2019/06/19/RS-GAN/2019-06-24-20-09-20.png"></p><hr><p><img src="/2019/06/19/RS-GAN/2019-06-24-20-10-51.png"></p><hr><p><img src="/2019/06/19/RS-GAN/2019-06-24-20-21-08.png"></p><hr><h2 id="d-optimise">D-optimise</h2><p><img src="/2019/06/19/RS-GAN/2019-06-24-20-10-18.png"></p><hr><p><img src="/2019/06/19/RS-GAN/2019-06-24-18-04-22.png"></p><hr><p><img src="/2019/06/19/RS-GAN/2019-06-24-18-07-34.png"></p><hr><p><img src="/2019/06/19/RS-GAN/2019-06-24-20-28-37.png"></p><hr><p><img src="/2019/06/19/RS-GAN/2019-06-24-20-28-55.png"></p><hr><p><img src="/2019/06/19/RS-GAN/2019-06-24-20-29-06.png"></p><hr><h1 id="cfgan">CFGAN</h1><hr><p><img src="/2019/06/19/RS-GAN/2019-06-24-18-09-59.png"></p><hr><p><img src="/2019/06/19/RS-GAN/2019-06-24-20-05-41.png"></p><hr><p><img src="/2019/06/19/RS-GAN/2019-06-24-18-11-03.png"></p><hr><p>Z:nosie C:User Vec</p><p>mask: mask向量eu中，用户交互过的地方为1，没有交互过的地方为0 - 为0的地方并不代表用户不喜欢，其次，用户没有购买过的物品，在乘上0之后，其实是训练的时候丢掉了这些节点，因为乘上0之后，梯度永远为0，是不会反向传播回去的； - 如果把真实购买向量对应位置设置为0的话，判别器在这些地方也是没有损失的。这与传统的矩阵分解方法是非常类似的，即用用户购买过的地方去训练，随后预测用户对未购买过的物品的评分。</p><hr><p><img src="/2019/06/19/RS-GAN/2019-06-24-18-13-20.png"></p><p><img src="/2019/06/19/RS-GAN/2019-06-24-18-13-27.png"></p><hr><h3 id="cfganzr">CFGAN−ZR</h3><p>Negative Sampling <img src="/2019/06/19/RS-GAN/2019-06-24-18-16-08.png"></p><hr><h3 id="cfganpm">CFGAN−PM</h3><p>partial-mask G: P-&gt;1 N-&gt;0 <img src="/2019/06/19/RS-GAN/2019-06-24-18-17-15.png"></p><hr><h3 id="cfganzp">CFGAN−ZP</h3><p><img src="/2019/06/19/RS-GAN/2019-06-24-18-19-26.png"></p><hr><h1 id="graphgan">GraphGAN</h1><p><img src="/2019/06/19/RS-GAN/2019-06-24-20-43-37.png"></p><hr><p><img src="/2019/06/19/RS-GAN/2019-06-24-20-44-14.png"></p><hr><p><img src="/2019/06/19/RS-GAN/2019-06-24-20-45-03.png"> Sigmoid/SDNE any D</p><p><img src="/2019/06/19/RS-GAN/2019-06-24-20-46-06.png"></p><hr><h2 id="graph-softmax">Graph Softmax</h2><p>仅针对直接使用softmax作为G提供Graph-Softmax <img src="/2019/06/19/RS-GAN/2019-06-24-20-46-57.png"></p><hr><p><img src="/2019/06/19/RS-GAN/2019-06-24-20-51-48.png"></p><hr><p><img src="/2019/06/19/RS-GAN/2019-06-24-20-54-58.png"></p><hr><h3 id="proof">proof</h3><p><img src="/2019/06/19/RS-GAN/2019-06-24-20-59-27.png"></p><p>先到叶子节点，建立新树，向上走计算概率</p><p><img src="/2019/06/19/RS-GAN/2019-06-24-21-05-55.png"></p><hr><p><img src="/2019/06/19/RS-GAN/2019-06-24-20-54-04.png"></p><hr><h1 id="thoughts">Thoughts</h1><p>直接用GAN做补充样本是无用的</p><p>GAN1:一套G/D网络进行序列产生判别 作为模拟环境 GAN2:一套G/D网络进行购物券产生 作为购物券生成器</p><p>先用G1/D1拟合现有的分布 seqGAN seq-&gt;G1-&gt;full seq (i-c-i-i)-&gt;(i-c-i-i-c-i) D进行判别这个分布是否有效(真实)</p><hr><p>利用G网络产生购物券种类 利用D网络判别购物券产生的有效性 利用conditional GAN产生特定的购物券？</p><p>再用G2/D2拟合购物券 购物券存在embedding (store,catgrary,user,购物券的组合(平台券/档口券(store),方式，金额))-&gt;G2-&gt;-&gt;embedding</p><p>[(store,catgrary,user,购物券的组合(平台券/档口券(store),方式，金额)),embedding] -&gt;D2-&gt;true/false<br>real embedding()-&gt;D2-&gt;true</p><hr><p><strong>真实</strong>： D2网络判别为真实购物券<br>D1网络判别序列是否真实<br><strong>有效</strong>： 购买时间--<br>购买金额++</p><h2 id="问题">问题</h2><ul><li>商家从来没这么发过购物券，是否会真实判定有问题<br></li><li>是否不需要GAN2 直接用MLP 生成购物券组合 embedding (梯度问题)</li></ul><hr><h2 id="reference">Reference</h2><blockquote><ul><li><a href>SeqGAN</a></li><li><a href>GraphGAN</a></li><li><a href>IRGAN</a></li><li><a href>CFGAN</a></li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;seqgan&quot;&gt;SeqGAN&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;/2019/06/19/RS-GAN/2019-06-24-16-38-17.png&quot;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Goal: generating sequences of discrete tok
      
    
    </summary>
    
      <category term="Recommendation_System" scheme="hoverwings.site/categories/Recommendation-System/"/>
    
    
  </entry>
  
  <entry>
    <title>RS_Explainability</title>
    <link href="hoverwings.site/2019/06/07/RS-Explainability/"/>
    <id>hoverwings.site/2019/06/07/RS-Explainability/</id>
    <published>2019-06-07T15:56:11.000Z</published>
    <updated>2019-06-19T17:34:29.940Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2019/06/07/RS-Explainability/ExplainableRS0.png" width="500px"></p><p><strong>PanXiang</strong> 2019.4.9</p><p><strong>Problem</strong></p><p>why such items are recommended</p><p>why they not only provideusers with the recommendations</p><p>effectiveness</p><p>efficiency</p><p>persuasiveness</p><p>user satisfaction</p><p>(how presented)display style:text/visual</p><p>(what information used)model:matrix factorization,topic modeling, graphbased models, deep learning, knowledge-graph embedding, association rule mining</p><p><strong>Time Line</strong></p><p><strong>CF(Memory-Based)</strong></p><p><strong>C</strong> <strong>ontent-</strong> <strong>B</strong> <strong>ased</strong> <strong>CF(feature based)</strong></p><p>(price, color, brand of the goods in e-commerce, or the genre, director, duration of the movies inreview systems)</p><p><strong>User-based CF</strong> <strong>(user embedding?)</strong></p><p><strong>Item-based CF(Item embedding?)</strong></p><p>MF(Model-Based)</p><p>生成式模型/判别式模型?</p><p><strong>MF</strong></p><p><strong>LFM(Latent Factor Model)</strong></p><p>m user</p><p>n item</p><p>A: user-(item favor value)</p><p>U: user-feature</p><p>V: feature-(itemfavor value)</p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS1.png" width="277px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS2.png" width="500px"></p><p><strong>MF</strong></p><p><strong>LFM(Latent Factor Model)</strong> <strong>/Singular Value Decomposition(SVD)?</strong></p><p>m user</p><p>n item</p><p>A: user-(item favor value)</p><p>U: user-feature</p><p>V: feature-(itemfavor value)</p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS3.png" width="277px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS4.png" width="451px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS5.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS6.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS7.png" width="227px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS8.png" width="500px"></p><p>yi反应喜好偏置</p><p>I(u)所有交互</p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS9.png" width="500px"></p><p>behaviour can reflect favor</p><p>2008 KDD Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model</p><p><strong>E</strong> <strong>FM(Explicit Factor Model)</strong></p><p>+explicit user opinions</p><p>explicit product features</p><p>user opinions by phrase-level sentiment analysis on user reviews</p><p>generate according to specific product features to the user’s interests and the hidden features learned</p><p>SIGIR 2014Explicit Factor Models for Explainable Recommendation based on Phrase-level Sentiment Analysis</p><p><strong>EFM(Explicit Factor Model)</strong></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS10.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS11.png" width="500px"></p><p>SIGIR 2014Explicit Factor Models for Explainable Recommendation based on Phrase-level Sentiment Analysis</p><p><strong>EFM(Explicit Factor Model)</strong></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS12.png" width="500px"></p><p>N=5 for five stars</p><p>feature Fj is mentioned by user ui for tij times</p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS13.png" width="500px"></p><p>Fj is mentioned for k times on item pi</p><p>SIGIR 2014Explicit Factor Models for Explainable Recommendation based on Phrase-level Sentiment Analysis</p><p><strong>EFM(Explicit Factor Model)</strong></p><p><strong>Explicit</strong> <strong>features</strong></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS14.png" width="500px"></p><p>feature: screen/earphone</p><p>factor: ???</p><p>X:user-feature attention matrix</p><p>Y:item-feature quality matrix</p><p>U1 user- factor</p><p>V feature-factor</p><p>U2 item- factor</p><p>r factor nums</p><p>m user</p><p>p feature</p><p>n item</p><p>SIGIR 2014Explicit Factor Models for Explainable Recommendation based on Phrase-level Sentiment Analysis</p><p><strong>EFM(Explicit Factor Model)</strong></p><p><strong>Latent</strong> <strong>f</strong> <strong>actors</strong></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS15.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS16.png" width="500px"></p><p>When r = 0, this model reduces to a traditional latent factorization model on user-item rating matrix A</p><p>which means that the explicit features are not used for recommendations</p><p><strong>EFM(Explicit Factor Model)</strong> <strong>+SVD++?</strong></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS17.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS18.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS19.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS20.png" width="410px"></p><p>SIGIR’18 Explainable Recommendation via Multi-Task Learning in Opinionated Text Data</p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS21.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS22.png" width="500px"></p><p><strong>ConvMF=CNN+PMF</strong></p><p><strong>PMF</strong></p><p>R: score Mat</p><p>posterior probability</p><p>M item</p><p>N doc</p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS23.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS24.png" width="481px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS25.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS26.png" width="156px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS27.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS28.png" width="128px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS29.png" width="129px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS30.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS31.png" width="381px"></p><p>2016Convolutional matrix factorization for document context-aware recommendation</p><p><strong>CNN:</strong></p><p><strong>(Music:Fourier-&gt;image-&gt;CNN predict</strong></p><p><strong>Genetrate Feature)</strong></p><p>Conv -Classification-&gt;feature extraction</p><p>MF -Regression</p><p>2016Convolutional matrix factorization for document context-aware recommendation</p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS32.png" width="500px"></p><p>Z[t]:update gate vector</p><p>R[t]:reset gate vector</p><p>RecSys17 Sequential User-based Recurrent Neural Network Recommendations</p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS33.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS34.png" width="491px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS35.png" width="500px"></p><p>RecSys17 Sequential User-based Recurrent Neural Network Recommendations</p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS36.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS37.png" width="500px"></p><p><strong>Text</strong></p><p><strong>Sentence:</strong></p><p><strong>Topic words:(features)</strong></p><p><strong>Visual</strong></p><p><strong>Friends</strong></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS38.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS39.png" width="473px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS40.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS41.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS42.png" width="500px"></p><p>generated character by</p><p>character</p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS43.png" width="500px"></p><p>concatenated word</p><p>embeddings of user, item, and rating</p><p>2018Automatic Generation of Natural Language Explanations</p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS44.png" width="500px"></p><p><strong>Text-based(NLP)</strong></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS45.png" width="500px"></p><p>2018Automatic Generation of Natural Language Explanations</p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS46.png" width="500px"></p><p><strong>Text-based(NLP)</strong></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS47.png" width="500px"></p><p>2018Automatic Generation of Natural Language Explanations</p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS48.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS49.png" width="500px"></p><p><strong>Knowledge-base</strong> <strong>=path comlpetion</strong></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS50.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS51.png" width="500px"></p><p>2019Unifying Knowledge Graph Learning and Recommendation: Towards a Better Understanding of User Preferences</p><p><strong>Knowledge-base</strong></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS52.png" width="500px"></p><p>AAAI2019Unifying Knowledge Graph Learning and Recommendation: Towards a Better Understanding of User Preferences</p><p><strong>Knowledge-base</strong></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS53.png" width="500px"></p><p>AAAI2019Unifying Knowledge Graph Learning and Recommendation: Towards a Better Understanding of User Preferences</p><p><strong>Knowledge-base</strong></p><p>TransE</p><p>TransH</p><p>It assumes that each relation</p><p>owns a hyperplane, and the translation between head entity and</p><p>tail entity is valid only if they are projected to the same hyperplane</p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS54.png" width="139px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS55.png" width="176px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS56.png" width="500px"></p><p>WWW2019Unifying Knowledge Graph Learning and Recommendation:Towards a Better Understanding of User Preferences</p><p><strong>Knowledge-base</strong></p><p>TransH</p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS57.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS58.png" width="242px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS59.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS60.png" width="500px"></p><p>WWW2019Unifying Knowledge Graph Learning and Recommendation:Towards a Better Understanding of User Preferences</p><p><strong>Evaluation-RS</strong></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS61.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS62.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS63.png" width="500px"></p><p>reli = 1 if the i-th element is a positive item</p><p><strong>Evaluation</strong> <strong>-Explainable</strong></p><p>explainability</p><p>EP:explainability precision</p><p>ER:explainabilityrecall</p><p>Text:</p><p>BLUE: 不同长度的统计值差异计算</p><p>Rough(Recall-Oriented Understudy for Gisting Evaluation)</p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS64.png" width="500px"></p><p>LCSLongest Child String</p><p>Open Question</p><p>DL explainability</p><p>Heterogenous Information Modeling</p><p>Text/Image/Audio Search</p><p>Natural Language Generation for Explanation</p><p>QA System</p><p>Explanation beyond Persuasiveness</p><p>letting the user know why not to buy a certain product</p><p>the system can help to save time for the users and to win user’s trustin the system</p><p>Open Question</p><p>Aggregation of Different Explanations</p><p><strong>Explainable ML</strong></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS65.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS66.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS67.png" width="500px"></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS68.png" width="500px"></p><p><strong>Explainable</strong> <strong>D</strong> <strong>L</strong></p><p><img src="/2019/06/07/RS-Explainability/ExplainableRS69.png" width="500px"></p><p>Understanding Black-box Predictions via Influence Functions</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/2019/06/07/RS-Explainability/ExplainableRS0.png&quot; width=&quot;500px&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PanXiang&lt;/strong&gt; 2019.4.9&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Probl
      
    
    </summary>
    
      <category term="Recommendation_System" scheme="hoverwings.site/categories/Recommendation-System/"/>
    
    
  </entry>
  
  <entry>
    <title>ML in Complex Netetwork</title>
    <link href="hoverwings.site/2019/06/04/ML-in-Complex-Netetwork/"/>
    <id>hoverwings.site/2019/06/04/ML-in-Complex-Netetwork/</id>
    <published>2019-06-04T06:34:18.000Z</published>
    <updated>2019-06-29T09:42:08.069Z</updated>
    
    <content type="html"><![CDATA[<!-- $size: 4:3 --><!-- page_number: true --><center><font size="11">Machine Learning in <br> Complex Network </font></center><p>Xiang Pan<br>Supervised By: Hong Huang<br>6/2/2019 ---</p><h1 id="complex-network">Complex Network</h1><p>A <strong>Complex Network</strong> is a graph (network) with non-trivial topological features—features that do not occur in simple networks such as lattices or random graphs but often occur in graphs modelling of real systems.</p><p><strong>Node and Link</strong>:The general model for the world <strong>CS</strong>?:Compute and Storage</p><hr><h2 id="traditional-method">Traditional Method</h2><ul><li>Dynamics<ul><li>Deep Walk<ul><li>Population Predication</li></ul></li><li>Propagation Theory<ul><li>Infectious Diseases Propagation</li></ul></li></ul></li><li>Cluster Theory<ul><li>Aggregation<ul><li>Community Dection</li></ul></li></ul></li></ul><h2 id="mlcomplex-network">ML+Complex Network</h2><ul><li>More information than Vector<ul><li>Build and represent do not loss critical information<ul><li>porpertity</li><li>structure</li></ul></li><li>Adapt to more general dataset type</li></ul></li><li>More Method<ul><li><strong>Graph CNN/DNN/VAE/GAN</strong></li></ul></li></ul><hr><p><img src="/2019/06/04/ML-in-Complex-Netetwork/2019-06-03-16-38-16.png"></p><!-- <img src="./ML-in-Complex-Netetwork/2019-06-03-16-38-16.png" width = "400" height = "400" div align= center /> --><hr><h1 id="pipeline">Pipeline</h1><h2 id="ml-pipeline">ML PipeLine</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">digraph G</span><br><span class="line">&#123;</span><br><span class="line">    &quot;Select Topic&quot;-&gt;&quot;Preparing Data Set(Data Clean)&quot;-&gt;&quot;Feature Engineering&quot;-&gt;&quot;Bilud Model&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="build-network">Build Network</h1><h2 id="defination">Defination</h2><h3 id="node">Node</h3><p><span class="math display">\[N=O(Node)\]</span> can be the anything of the world The understanding</p><hr><h3 id="link">Link</h3><h4 id="similarity-and-diffidence">Similarity And Diffidence</h4><p><span class="math display">\[{ V*V \in R  }\]</span></p><p><span class="math display">\[{ S_{ij}=s(v_i,v_j)  }\]</span></p><p>The <strong>Similarity Function</strong> can be the diastance of vector representation <span class="math display">\[{ D_{ij}=d(v_i,v_j)  }\]</span></p><hr><p>The <strong>Diffidence Function</strong> can be calculated by the Similarity Function <span class="math display">\[{    d(v_i,v_j)=\sqrt {s(v_i,v_i)+s(v_j,v_j)-2s(v_i,v_j)}}\]</span></p><p><span class="math display">\[{ P_{ij}=\alpha * S_{ij} + \beta *1/D_{ij}       }\]</span></p><p>Then set a <strong>threshhold</strong> to build the link</p><hr><h4 id="how-to-determine-the-threshholdhyperparameter">How to determine the threshhold(hyperparameter)?</h4><ul><li>Network sparsity(Experience) &gt;After obtaining the base image matrix and coefficient matrix of non-negative matrix factorization (NMF), Hoyer [1] proposed that the degree of difference between the L1 norm and the L2 norm can be used to measure the sparsity of the matrix after decomposition.</li><li>Training result<ul><li>Auto ML</li></ul></li></ul><h1 id="representataion">Representataion</h1><p><strong>Network Representation</strong>:Using some method to reduce the dimension of the data, then the network can be represent as a <strong>Vector</strong> - <strong>Graph Embedding</strong></p><hr><h2 id="mf">MF</h2><p>M is an <strong>Orthogonal matrix</strong> ### SVD <img src="/2019/06/04/ML-in-Complex-Netetwork/2019-06-03-17-47-20.png"></p><hr><p><span class="math display">\[M \in Space_{node * node}\]</span></p><p><span class="math inline">\(U\)</span> is the main Vector representation <span class="math inline">\(V= \Sigma * V^*\)</span> is the Background Vector Repersentation</p><hr><h2 id="deep-walk">Deep Walk</h2><p><img src="/2019/06/04/ML-in-Complex-Netetwork/2019-06-03-17-02-28.png"></p><h4 id="hierarchical-huffman-tree">Hierarchical Huffman Tree</h4><hr><h3 id="can-be-understood-by-mf">Can be understood by MF?</h3><p><img src="/2019/06/04/ML-in-Complex-Netetwork/2019-06-03-17-11-02.png"></p><hr><p><img src="/2019/06/04/ML-in-Complex-Netetwork/2019-06-03-17-11-38.png"></p><hr><p><img src="/2019/06/04/ML-in-Complex-Netetwork/2019-06-03-17-24-32.png"></p><hr><p><img src="/2019/06/04/ML-in-Complex-Netetwork/2019-06-03-17-10-07.png"></p><p><img src="/2019/06/04/ML-in-Complex-Netetwork/2019-06-03-17-25-36.png"></p><h2 id="pte">PTE</h2><p><img src="/2019/06/04/ML-in-Complex-Netetwork/2019-06-03-17-27-13.png"></p><h2 id="node2vec-2nd-order-random-walk">node2vec — 2nd Order Random Walk</h2><p><img src="/2019/06/04/ML-in-Complex-Netetwork/2019-06-03-17-28-42.png"></p><hr><h3 id="node2vec-as-implicit-matrix-factorization">node2vec as Implicit Matrix Factorization</h3><p><img src="/2019/06/04/ML-in-Complex-Netetwork/2019-06-03-17-29-10.png"></p><hr><h2 id="everything-is-mf">Everything is MF</h2><p><img src="/2019/06/04/ML-in-Complex-Netetwork/2019-06-03-16-54-40.png"></p><hr><h1 id="netmf">NetMF</h1><p><img src="/2019/06/04/ML-in-Complex-Netetwork/2019-06-03-17-30-24.png"></p><hr><p><img src="/2019/06/04/ML-in-Complex-Netetwork/2019-06-03-17-30-45.png"></p><hr><p><img src="/2019/06/04/ML-in-Complex-Netetwork/2019-06-03-17-31-06.png"></p><h1 id="application">Application</h1><h2 id="abstract">Abstract</h2><ul><li>Link Prediction</li><li>Clustering</li><li>Dynamic Network Analysis</li></ul><h2 id="real-world">Real World</h2><ul><li>Infectious Diseases</li><li>Academic Network</li><li>Media Web<ul><li><strong>Recommendation System(RS)</strong></li><li>Terrorist Dection</li></ul></li></ul><hr><h1 id="recommendation-system">Recommendation System</h1><p><img src="/2019/06/04/ML-in-Complex-Netetwork/2019-06-03-17-33-45.png"></p><hr><p><img src="/2019/06/04/ML-in-Complex-Netetwork/2019-06-03-17-34-02.png"></p><hr><h1 id="reference">Reference</h1><ul><li><a href="https://www.aminer.cn/nrl_www2019" target="_blank" rel="noopener">Jie Tang's Tutorial in WWW 2019</a></li><li><a href>Machine Leaning in Complex Network</a></li></ul><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- $size: 4:3 --&gt;
&lt;!-- page_number: true --&gt;
&lt;center&gt;
&lt;font size=&quot;11&quot;&gt;Machine Learning in &lt;br&gt; Complex Network &lt;/font&gt;
&lt;/center&gt;
&lt;p&gt;Xiang 
      
    
    </summary>
    
      <category term="Graph_Network" scheme="hoverwings.site/categories/Graph-Network/"/>
    
    
  </entry>
  
  <entry>
    <title>RS_Transformer</title>
    <link href="hoverwings.site/2019/06/04/RS-Transformer/"/>
    <id>hoverwings.site/2019/06/04/RS-Transformer/</id>
    <published>2019-06-03T18:25:12.000Z</published>
    <updated>2019-06-19T17:34:29.940Z</updated>
    
    <content type="html"><![CDATA[<h1 id="behavior-sequence-transformer-for-e-commerce-recommendation-in-alibaba">Behavior Sequence Transformer for E-commerce Recommendation in Alibaba</h1><p><img src="/2019/06/04/RS-Transformer/2019-06-04-01-10-36.png"></p><hr><h1 id="abstract">Abstract</h1><ul><li><strong>Embedding&amp;MLP paradigm</strong>: raw features are embedded into lowdimensional vectors<ul><li>ignoring the sequential nature of users’ behaviors</li></ul></li><li><strong>Transformer model</strong> to capture the sequential signals underlying users’ behavior sequences for recommendation</li></ul><hr><h1 id="transforemerattention-is-all-you-need">Transforemer(Attention is all you need)</h1><h2 id="architecture">Architecture</h2><p><img src="/2019/06/04/RS-Transformer/2019-06-04-01-16-51.png"></p><hr><p><img src="/2019/06/04/RS-Transformer/2019-06-04-01-17-12.png"></p><hr><p><img src="/2019/06/04/RS-Transformer/2019-06-04-01-18-04.png"></p><p>Every encoder do not share their weights</p><hr><h2 id="self-attention-encoder">Self-Attention Encoder</h2><p>Input Word-&gt;Embededing <span class="math display">\[ q_1=V*W_q \]</span> <span class="math display">\[ k_1=V*W_k \]</span> <span class="math display">\[ v_1=V*W_v \]</span></p><p><img src="/2019/06/04/RS-Transformer/2019-06-04-01-30-16.png"></p><hr><p><img src="/2019/06/04/RS-Transformer/2019-06-04-01-31-21.png"></p><hr><p><strong>Position</strong>: diffierent distance to different pos</p><p><img src="/2019/06/04/RS-Transformer/2019-06-04-01-34-53.png"></p><hr><h2 id="decoder">Decoder</h2><p>The Unis number is O(Word)<br><img src="/2019/06/04/RS-Transformer/2019-06-04-01-39-58.png"></p><hr><p><img src="/2019/06/04/RS-Transformer/2019-06-04-01-36-45.png"></p><hr><h2 id="loss">loss</h2><p>The differience of 2 distribution - Cross Entropy - KL Distance</p><hr><h1 id="bst">BST</h1><h2 id="architecture-1">Architecture</h2><p><img src="/2019/06/04/RS-Transformer/2019-06-04-01-45-45.png"></p><hr><h2 id="feature">Feature</h2><h3 id="general-embedding">General Embedding</h3><p><img src="/2019/06/04/RS-Transformer/2019-06-04-01-46-58.png"></p><hr><h3 id="positional-embedding">Positional embedding</h3><p><span class="math display">\[pos(v_i) = t(v_t) − t(v_i)\]</span></p><p>where <span class="math inline">\(t(v_t)\)</span> represents the recommending time and <span class="math inline">\(t(v_i)\)</span> the timestamp when user click item <span class="math inline">\(v_i\)</span>.</p><hr><h2 id="transformer">Transformer</h2><h3 id="self-attention-layer">Self-attention layer</h3><p><img src="/2019/06/04/RS-Transformer/2019-06-04-01-49-56.png"></p><hr><p><img src="/2019/06/04/RS-Transformer/2019-06-04-01-50-42.png"></p><hr><h3 id="point-wise-feed-forward-networks">Point-wise Feed-Forward Networks</h3><p><img src="/2019/06/04/RS-Transformer/2019-06-04-01-51-23.png"></p><hr><h2 id="mlp-layers-and-loss-function">MLP layers and Loss function</h2><p><strong>binary classification problem</strong><br><img src="/2019/06/04/RS-Transformer/2019-06-04-01-52-08.png"></p><hr><h1 id="result">Result</h1><p><img src="/2019/06/04/RS-Transformer/2019-06-04-01-53-02.png"> <img src="/2019/06/04/RS-Transformer/2019-06-04-01-53-15.png"></p><hr><p><img src="/2019/06/04/RS-Transformer/2019-06-04-01-53-32.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;behavior-sequence-transformer-for-e-commerce-recommendation-in-alibaba&quot;&gt;Behavior Sequence Transformer for E-commerce Recommendation 
      
    
    </summary>
    
      <category term="Recommendation_System" scheme="hoverwings.site/categories/Recommendation-System/"/>
    
    
  </entry>
  
</feed>
